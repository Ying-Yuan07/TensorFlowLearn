{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "app使用推荐Attention_on_LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO1gncRfM41o0nx6iraXahW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ying-Yuan07/TensorFlowLearn/blob/main/app%E4%BD%BF%E7%94%A8%E6%8E%A8%E8%8D%90Attention_on_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AI0Ol_GXnwKQ",
        "outputId": "67ec6886-309f-4a90-bea1-04e77acf8801"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: keras==2.1.6 in /usr/local/lib/python3.7/dist-packages (2.1.6)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras==2.1.6) (1.7.3)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras==2.1.6) (3.1.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras==2.1.6) (3.13)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from keras==2.1.6) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras==2.1.6) (1.21.6)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras==2.1.6) (1.5.2)\n",
            "train_data.shape, test_data.shape\n",
            "(43094, 3) (18470, 3)\n",
            "item_seq_train.shape\n",
            "(6,)\n",
            "item_seq_test.shape\n",
            "(6,)\n",
            "len(entire_products)\n",
            "17\n",
            "entire_products\n",
            "['Google', 'Maps', 'Google Chrome', 'Gmail', 'Facebook', 'Facebook Messenger', 'Instagram', 'Google Play Store', 'Twitter', 'Samsung Gallery', 'YouTube', 'Yahoo Mail', 'Pinterest', 'Contacts', 'Amazon Shopping', 'WhatsApp Messenger', 'Netflix']\n",
            "train_x_emb.shape\n",
            "(43064, 5, 10)\n",
            "train_x.shape\n",
            "(43064, 5, 10)\n",
            "train_y shape\n",
            "(43064,)\n",
            "after one_hot train_y,total_vocab\n",
            "(43064, 17)\n",
            "model is building\n",
            "Epoch 1/50\n",
            "43064/43064 [==============================] - 11s 255us/step - loss: 1.6251 - acc: 0.5242\n",
            "Epoch 2/50\n",
            "43064/43064 [==============================] - 7s 152us/step - loss: 1.4222 - acc: 0.5800\n",
            "Epoch 3/50\n",
            "43064/43064 [==============================] - 7s 154us/step - loss: 1.3961 - acc: 0.5893\n",
            "Epoch 4/50\n",
            "43064/43064 [==============================] - 6s 144us/step - loss: 1.3801 - acc: 0.5934\n",
            "Epoch 5/50\n",
            "43064/43064 [==============================] - 7s 164us/step - loss: 1.3694 - acc: 0.5970\n",
            "Epoch 6/50\n",
            "43064/43064 [==============================] - 6s 147us/step - loss: 1.3606 - acc: 0.5974\n",
            "Epoch 7/50\n",
            "43064/43064 [==============================] - 7s 174us/step - loss: 1.3542 - acc: 0.6002\n",
            "Epoch 8/50\n",
            "43064/43064 [==============================] - 9s 200us/step - loss: 1.3484 - acc: 0.6013\n",
            "Epoch 9/50\n",
            "43064/43064 [==============================] - 7s 161us/step - loss: 1.3437 - acc: 0.6027\n",
            "Epoch 10/50\n",
            "43064/43064 [==============================] - 6s 142us/step - loss: 1.3399 - acc: 0.6034\n",
            "Epoch 11/50\n",
            "43064/43064 [==============================] - 8s 176us/step - loss: 1.3362 - acc: 0.6040\n",
            "Epoch 12/50\n",
            "43064/43064 [==============================] - 6s 138us/step - loss: 1.3335 - acc: 0.6051\n",
            "Epoch 13/50\n",
            "43064/43064 [==============================] - 6s 146us/step - loss: 1.3308 - acc: 0.6061\n",
            "Epoch 14/50\n",
            "43064/43064 [==============================] - 5s 112us/step - loss: 1.3284 - acc: 0.6062\n",
            "Epoch 15/50\n",
            "43064/43064 [==============================] - 5s 125us/step - loss: 1.3265 - acc: 0.6058\n",
            "Epoch 16/50\n",
            "43064/43064 [==============================] - 5s 114us/step - loss: 1.3238 - acc: 0.6075\n",
            "Epoch 17/50\n",
            "43064/43064 [==============================] - 5s 110us/step - loss: 1.3225 - acc: 0.6069\n",
            "Epoch 18/50\n",
            "43064/43064 [==============================] - 5s 107us/step - loss: 1.3203 - acc: 0.6071\n",
            "Epoch 19/50\n",
            "43064/43064 [==============================] - 7s 165us/step - loss: 1.3189 - acc: 0.6084\n",
            "Epoch 20/50\n",
            "43064/43064 [==============================] - 7s 170us/step - loss: 1.3171 - acc: 0.6080\n",
            "Epoch 21/50\n",
            "43064/43064 [==============================] - 5s 107us/step - loss: 1.3157 - acc: 0.6082\n",
            "Epoch 22/50\n",
            "43064/43064 [==============================] - 5s 107us/step - loss: 1.3143 - acc: 0.6087\n",
            "Epoch 23/50\n",
            "43064/43064 [==============================] - 5s 107us/step - loss: 1.3131 - acc: 0.6095\n",
            "Epoch 24/50\n",
            "43064/43064 [==============================] - 5s 105us/step - loss: 1.3115 - acc: 0.6084\n",
            "Epoch 25/50\n",
            "43064/43064 [==============================] - 5s 120us/step - loss: 1.3107 - acc: 0.6087\n",
            "Epoch 26/50\n",
            "43064/43064 [==============================] - 5s 107us/step - loss: 1.3092 - acc: 0.6104\n",
            "Epoch 27/50\n",
            "43064/43064 [==============================] - 5s 115us/step - loss: 1.3077 - acc: 0.6105\n",
            "Epoch 28/50\n",
            "43064/43064 [==============================] - 5s 107us/step - loss: 1.3065 - acc: 0.6114\n",
            "Epoch 29/50\n",
            "43064/43064 [==============================] - 5s 108us/step - loss: 1.3055 - acc: 0.6103\n",
            "Epoch 30/50\n",
            "43064/43064 [==============================] - 5s 105us/step - loss: 1.3042 - acc: 0.6123\n",
            "Epoch 31/50\n",
            "43064/43064 [==============================] - 5s 106us/step - loss: 1.3027 - acc: 0.6120\n",
            "Epoch 32/50\n",
            "43064/43064 [==============================] - 5s 107us/step - loss: 1.3022 - acc: 0.6115\n",
            "Epoch 33/50\n",
            "43064/43064 [==============================] - 5s 107us/step - loss: 1.3010 - acc: 0.6123\n",
            "Epoch 34/50\n",
            "43064/43064 [==============================] - 5s 108us/step - loss: 1.2994 - acc: 0.6113\n",
            "Epoch 35/50\n",
            "43064/43064 [==============================] - 5s 105us/step - loss: 1.2984 - acc: 0.6119\n",
            "Epoch 36/50\n",
            "43064/43064 [==============================] - 5s 108us/step - loss: 1.2974 - acc: 0.6120\n",
            "Epoch 37/50\n",
            "43064/43064 [==============================] - 5s 120us/step - loss: 1.2961 - acc: 0.6126\n",
            "Epoch 38/50\n",
            "43064/43064 [==============================] - 5s 108us/step - loss: 1.2951 - acc: 0.6134\n",
            "Epoch 39/50\n",
            "43064/43064 [==============================] - 5s 109us/step - loss: 1.2940 - acc: 0.6127\n",
            "Epoch 40/50\n",
            "43064/43064 [==============================] - 5s 110us/step - loss: 1.2937 - acc: 0.6135\n",
            "Epoch 41/50\n",
            "43064/43064 [==============================] - 5s 109us/step - loss: 1.2925 - acc: 0.6139\n",
            "Epoch 42/50\n",
            "43064/43064 [==============================] - 7s 158us/step - loss: 1.2914 - acc: 0.6144\n",
            "Epoch 43/50\n",
            "43064/43064 [==============================] - 8s 192us/step - loss: 1.2905 - acc: 0.6138\n",
            "Epoch 44/50\n",
            "43064/43064 [==============================] - 8s 181us/step - loss: 1.2896 - acc: 0.6152\n",
            "Epoch 45/50\n",
            "43064/43064 [==============================] - 7s 173us/step - loss: 1.2888 - acc: 0.6144\n",
            "Epoch 46/50\n",
            "43064/43064 [==============================] - 8s 176us/step - loss: 1.2876 - acc: 0.6146\n",
            "Epoch 47/50\n",
            "43064/43064 [==============================] - 8s 178us/step - loss: 1.2867 - acc: 0.6149\n",
            "Epoch 48/50\n",
            "43064/43064 [==============================] - 5s 111us/step - loss: 1.2860 - acc: 0.6152\n",
            "Epoch 49/50\n",
            "43064/43064 [==============================] - 5s 119us/step - loss: 1.2851 - acc: 0.6158\n",
            "Epoch 50/50\n",
            "43064/43064 [==============================] - 6s 138us/step - loss: 1.2844 - acc: 0.6143\n",
            "model building done\n",
            "pred.shape\n",
            "(18440, 17)\n",
            "preddy.shape\n",
            "(18440,)\n",
            "hit_rate_at_1(preddy,test_seq)\n",
            "0.5355748373101952\n",
            "hit_rate_at_5(preddy,test_seq)\n",
            "0.8967462039045553\n",
            "hit_rate_at_10(preddy,test_seq)\n",
            "0.9669197396963124\n",
            "Done\n"
          ]
        }
      ],
      "source": [
        "%tensorflow_version 1.x\n",
        "!pip install keras==2.1.6\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from gensim.models import Word2Vec\n",
        "from keras.layers import Input,Dense,LSTM,Activation,RepeatVector,Permute,Flatten,Multiply\n",
        "from keras.models import Sequential\n",
        "from keras.models import load_model,Model\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "np.random.seed(123)\n",
        "\n",
        "#Read data into pandas dataframe\n",
        "df=pd.read_csv('./sample_data/username_100.csv')\n",
        "df=df[['week', 'hour_part','app_name']]\n",
        "\n",
        "#Rename the columns\n",
        "df.columns=['week', 'hour_part','app_name']\n",
        "\"\"\"\n",
        "df=df.sort_values(['reviewTime'],ascending=[True])\n",
        "\"\"\"\n",
        "#Divide the train and test data\n",
        "#取数据0-1500行为训练集，1500-2000为测试集\n",
        "\n",
        "length=int(df.shape[0]*0.7)\n",
        "test_end = 61560\n",
        "train_data=df[:length]\n",
        "test_data=df[length:]\n",
        "print(\"train_data.shape, test_data.shape\")\n",
        "print(train_data.shape, test_data.shape)\n",
        "train_data = train_data.reset_index()\n",
        "test_data = test_data.reset_index()\n",
        "    \n",
        "#find item sequences by user in train and test\n",
        "#按用户id排序，统计每个用户购买商品（item_id）的情况，将同一个用户购买的商品（item_id）放入同一个list，所有list组成一个array\n",
        "item_seq_train=train_data.groupby(\"hour_part\")['app_name'].apply(list).values\n",
        "item_seq_test=test_data.groupby(\"hour_part\")['app_name'].apply(list).values\n",
        "print(\"item_seq_train.shape\")\n",
        "print(item_seq_train.shape)\n",
        "print(\"item_seq_test.shape\")\n",
        "print(item_seq_test.shape)\n",
        "# We consider the minimum sequence length as 6. If length is less than 6, we append dummy ids to the sequence  \n",
        "# 一个用户购买商品数不足6个时，用'padding_id'填充到6个\n",
        "def min_six_len_seq(dictList):\n",
        "    new_list=[]\n",
        "    for i in range(0,len(dictList)):\n",
        "        if len(dictList[i])<6:\n",
        "            w=6-len(dictList[i])\n",
        "            dictList[i]=['app0']*w+dictList[i]\n",
        "    return dictList\n",
        "\n",
        "#find the train and test sequences of min len 6\n",
        "item_train = min_six_len_seq(item_seq_train)\n",
        "item_test = min_six_len_seq(item_seq_test)\n",
        "\n",
        "# train word2vec model on train_data to get item embeddings\n",
        "#Word2Vec(train_data,size,window,min_count,iter)\n",
        "#size是指词向量的维度，默认为100\n",
        "#min_count:要计算词向量的最小词频。这个值可以去掉一些很生僻的低频词，默认是5.可以对字典做截断， 词频少于min_count次数的单词会被丢弃掉\n",
        "#window:即词向量上下文最大距离,默认为5。window越大，则和某一词较远的词也会产生上下文关系，一般为[5,10]\n",
        "#iter: 随机梯度下降法中迭代的最大次数，默认是5。对于大语料，可以增大这个值\n",
        "#workers：用于控制训练的并行数。\n",
        "def word2vec_model(train_data):\n",
        "    model = Word2Vec(train_data,size=10,window = 3,min_count =1,iter=5)\n",
        "    return model\n",
        "\n",
        "\n",
        "#train the model and save\n",
        "wv_model=word2vec_model(item_train)\n",
        "wv_model.save('word2vec_model')\n",
        "\n",
        "#find full vocabulary\n",
        "entire_products=[]\n",
        "i = 0\n",
        "for key,value in wv_model.wv.vocab.items():\n",
        "    entire_products.append(key)\n",
        "np.save('./entire_products.npy',entire_products)\n",
        "print(\"len(entire_products)\")\n",
        "print(len(entire_products))\n",
        "print(\"entire_products\")\n",
        "print(entire_products)\n",
        "\n",
        "# Divide the sequences into length of 6. ( First 5 items are for train, 6th one for target)\n",
        "#用5个历史购买商品预测下一个商品\n",
        "def input_sequences(new_list, win_size=5):\n",
        "    input_seq=[]\n",
        "    target=[]\n",
        "    for i in range(0,len(new_list)):\n",
        "        seq_len = len(new_list[i])\n",
        "        for j in range(0,seq_len):\n",
        "            if j+win_size<seq_len:\n",
        "                if new_list[i][j+5] in entire_products:\n",
        "                    input_seq.append(new_list[i][j:j+5])\n",
        "                    target.append(new_list[i][j+5])\n",
        "    return input_seq,target\n",
        "\n",
        "# Encoding the target.If new item arrives which i\n",
        "# 将目标商品列表target(原始数据是商品id)转换成商品词汇库中的序号\n",
        "def num_products(target):\n",
        "    product_label=LabelEncoder()\n",
        "    product_label.fit(entire_products)\n",
        "    target_int = product_label.transform(target)\n",
        "    return target_int\n",
        "    \n",
        "#Create Train and test input and target sequences\n",
        "#train_x:5个历史购买商品id list组成的arrary,train_y:目标商品在商品词汇库中对应的序号list\n",
        "train_x,target_train=input_sequences(item_seq_train)\n",
        "train_y=num_products(target_train)\n",
        "\n",
        "test_x,target_test=input_sequences(item_seq_test)\n",
        "test_y=num_products(target_test)\n",
        "    \n",
        "#represent each item with prod2vec embedding and if new item comes in test set, represent it with random vec\n",
        "#将train_x中的每一个商品id都映射成一个维度为10的向量\n",
        "unknown_item_id=np.random.random((10,))\n",
        "def w2v_data_extraction(new_list):\n",
        "    w2v_data=[]\n",
        "    for i in range(0,len(new_list)):\n",
        "        seq_vec=[]\n",
        "        for j in range(0,len(new_list[i])):\n",
        "            try:\n",
        "                embedding=wv_model.wv[new_list[i][j]]\n",
        "            except KeyError:\n",
        "                embedding=unknown_item_id\n",
        "            seq_vec.append(embedding)\n",
        "                \n",
        "        w2v_data.append(seq_vec)\n",
        "    return np.asarray(w2v_data)\n",
        "\n",
        "train_x_emb=w2v_data_extraction(train_x)\n",
        "test_x_emb=w2v_data_extraction(test_x)\n",
        "print(\"train_x_emb.shape\")\n",
        "print(train_x_emb.shape)\n",
        "#model architecture\n",
        "def model_arch():\n",
        "    main_input = Input(shape=(5,10), name='main_input')\n",
        "    lstm_out = LSTM(32)(main_input)\n",
        "    attention = Dense(1, activation='tanh')(lstm_out)\n",
        "    attention = Activation('softmax')(attention)\n",
        "    attention = RepeatVector(32)(attention)\n",
        "    attention = Permute([2, 1])(attention)\n",
        "    attention = Flatten()(attention)\n",
        "    attention_mul = Multiply()([lstm_out, attention])\n",
        "    main_output = (Dense(total_vocab, activation='softmax', name='main_output')(attention_mul))\n",
        "    model = Model(inputs=main_input, outputs=main_output)\n",
        "    model.compile(loss='categorical_crossentropy',metrics=['accuracy'],optimizer='ADAM')\n",
        "    return model\n",
        "\n",
        "\n",
        "#represent output as one-hot encoded\n",
        "def one_hot(seq,total_vocab):\n",
        "    seq_one_hot=np.zeros([len(seq),total_vocab])\n",
        "    for i in range(0,len(seq)):\n",
        "        seq_one_hot[i][seq[i]]=1\n",
        "    return seq_one_hot\n",
        "\n",
        "#fit the model on our data\n",
        "def model_fit(model,train_x,train_y,total_vocab):\n",
        "    print(\"train_x.shape\")\n",
        "    print(train_x.shape)\n",
        "    print(\"train_y shape\")\n",
        "    print(train_y.shape)\n",
        "    train_y=one_hot(train_y,total_vocab)\n",
        "    print(\"after one_hot train_y,total_vocab\")\n",
        "    print(train_y.shape)\n",
        "\n",
        "    print(\"model is building\")\n",
        "    model.fit(batch_size=64,epochs=50,x=train_x,y=train_y)\n",
        "    print(\"model building done\")\n",
        "    model.save('keras_model.h5')\n",
        "    \n",
        "    return model\n",
        "\n",
        "\n",
        "total_vocab=len(entire_products)\n",
        "model = model_arch()\n",
        "model=model_fit(model,train_x_emb,train_y,total_vocab)\n",
        "\n",
        "# Hit rate at 1 on test data\n",
        "def hit_rate_at_1(prediction,actual):\n",
        "    return accuracy_score(prediction,actual)\n",
        "\n",
        "\n",
        "# 召回率 将正类预测为正类/原本的正类 = TP/(TP+FP)\n",
        "# Hit rata at 5 on test data\n",
        "# pred 返回的是一个n行k列的数组，行代表样本，j代表标签\n",
        "# 第i行第j列上的数值是模型预测第i个预测样本为第j个标签的概率，并且每一行的概率和为1\n",
        "def hit_rate_at_5(pred,actual):\n",
        "    predics = []\n",
        "    for i in range(0, len(pred)):\n",
        "        predics.append(np.argsort(pred[i])[-5:])#取出第i行，数值top5的下标放入predics，即筛选第i个样本预测概率top5的目标索引\n",
        "    count = 0\n",
        "    for i in range(0, len(predics)):#在n个样本预测的结果中，命中的次数\n",
        "        if actual[i] in predics[i]:\n",
        "            count = count + 1\n",
        "    return count/len(actual)\n",
        "\n",
        "\n",
        "# Hit rate at 10 on test data\n",
        "def hit_rate_at_10(pred, actual):\n",
        "    predics = []\n",
        "    for i in range(0, len(pred)):\n",
        "        predics.append(np.argsort(pred[i])[-10:])\n",
        "    count = 0\n",
        "    for i in range(0, len(predics)):\n",
        "        if actual[i] in predics[i]:\n",
        "            count = count + 1\n",
        "    return count /len(actual)\n",
        "    \n",
        "# Prediction on test data\n",
        "def model_predict(model,test_x,test_seq):\n",
        "\n",
        "    pred=model.predict(x=test_x)\n",
        "    preddy=np.argmax(a=pred,axis=1)\n",
        "\n",
        "    print(\"pred.shape\")\n",
        "    print(pred.shape)\n",
        "    print(\"preddy.shape\")\n",
        "    print(preddy.shape)\n",
        "\n",
        "    print(\"hit_rate_at_1(preddy,test_seq)\")\n",
        "    print(hit_rate_at_1(preddy,test_seq))\n",
        "    print(\"hit_rate_at_5(preddy,test_seq)\")\n",
        "    print(hit_rate_at_5(pred, test_seq))\n",
        "    print(\"hit_rate_at_10(preddy,test_seq)\")\n",
        "    print(hit_rate_at_10(pred, test_seq))\n",
        "\n",
        "#predict on test data\n",
        "model_predict(model,test_x_emb,test_y)\n",
        "print(\"Done\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "MbTzrs8R2WWO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#load entire_products\n",
        "loadData = np.load('entire_products.npy')\n",
        "print(loadData.shape)\n",
        "print(loadData)\n",
        "\n",
        "for i in range(len(entire_products)):\n",
        "  if(loadData[i] == entire_products[i]):\n",
        "    print(i)\n",
        "    print(loadData[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "id": "kt7ASntwmSXh",
        "outputId": "29d89136-dcf2-41ac-c30a-bdf225061bc3"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-15-0f6c7e6365a2>\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    for i in range(len(entire_products)):\u001b[0m\n\u001b[0m                                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#load word2vec_model\n",
        "w2v_model = Word2Vec.load('word2vec_model')\n",
        "mobile_test_x = ['Google Chrome','Yahoo Mail','Yahoo Mail','Google Chrome','Yahoo Mail']\n",
        "mobile_test_y = ['Yahoo Mail']"
      ],
      "metadata": {
        "id": "XgJNWHN5qIWw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}