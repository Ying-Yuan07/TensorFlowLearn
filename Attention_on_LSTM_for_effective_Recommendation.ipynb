{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Attention-on-LSTM-for-effective-Recommendation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM7FeZxW6+ufpoiZHlxAVlN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ying-Yuan07/TensorFlowLearn/blob/main/Attention_on_LSTM_for_effective_Recommendation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ldBBiNmEZq8m",
        "outputId": "d1107978-6d10-4c31-a4d8-44f0016b5869"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: keras==2.1.6 in /usr/local/lib/python3.7/dist-packages (2.1.6)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from keras==2.1.6) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras==2.1.6) (1.21.6)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras==2.1.6) (3.13)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras==2.1.6) (1.4.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras==2.1.6) (3.1.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras==2.1.6) (1.5.2)\n",
            "train_data.shape, test_data.shape\n",
            "(1500, 4) (500, 4)\n",
            "item_seq_train.shape\n",
            "(1196,)\n",
            "item_seq_test.shape\n",
            "(419,)\n",
            "item_seq_test[0]\n",
            "['B0002VETQS']\n",
            "item_test[5]\n",
            "['padding_id', 'padding_id', 'padding_id', 'padding_id', 'padding_id', 'B0009OAGXI']\n",
            "train_x_emb.shape\n",
            "(1327, 5, 50)\n",
            "train_x.shape\n",
            "(1327, 5, 50)\n",
            "train_y\n",
            "(1327,)\n",
            "after one_hot train_y,total_vocab\n",
            "(1327, 12102)\n",
            "12102\n",
            "model is building\n",
            "Epoch 1/10\n",
            "1327/1327 [==============================] - 5s 4ms/step - loss: 9.3806 - acc: 0.0090\n",
            "Epoch 2/10\n",
            "1327/1327 [==============================] - 1s 715us/step - loss: 9.0181 - acc: 0.0234\n",
            "Epoch 3/10\n",
            "1327/1327 [==============================] - 1s 683us/step - loss: 7.8480 - acc: 0.0234\n",
            "Epoch 4/10\n",
            "1327/1327 [==============================] - 1s 697us/step - loss: 6.8403 - acc: 0.0234\n",
            "Epoch 5/10\n",
            "1327/1327 [==============================] - 1s 679us/step - loss: 6.5132 - acc: 0.0234\n",
            "Epoch 6/10\n",
            "1327/1327 [==============================] - 1s 680us/step - loss: 6.3918 - acc: 0.0226\n",
            "Epoch 7/10\n",
            "1327/1327 [==============================] - 1s 676us/step - loss: 6.3378 - acc: 0.0211\n",
            "Epoch 8/10\n",
            "1327/1327 [==============================] - 1s 689us/step - loss: 6.3112 - acc: 0.0196\n",
            "Epoch 9/10\n",
            "1327/1327 [==============================] - 1s 660us/step - loss: 6.2917 - acc: 0.0226\n",
            "Epoch 10/10\n",
            "1327/1327 [==============================] - 1s 668us/step - loss: 6.2775 - acc: 0.0234\n",
            "model building done\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-59-00583a3c74aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[0;31m#predict on test data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m \u001b[0mmodel_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Done\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-59-00583a3c74aa>\u001b[0m in \u001b[0;36mmodel_predict\u001b[0;34m(model, test_x, test_seq)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;31m# Prediction on test data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmodel_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mpred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0mpreddy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1815\u001b[0m         x = _standardize_input_data(x, self._feed_input_names,\n\u001b[1;32m   1816\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1817\u001b[0;31m                                     check_batch_axis=False)\n\u001b[0m\u001b[1;32m   1818\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1819\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'DataFrame'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'DataFrame'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'ndim'"
          ]
        }
      ],
      "source": [
        "%tensorflow_version 1.x\n",
        "!pip install keras==2.1.6\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import keras.models as kmodels\n",
        "import keras.layers as klayers\n",
        "import keras.backend as K\n",
        "from gensim.models import Word2Vec\n",
        "from keras.layers import Input,Dense,LSTM,Activation,RepeatVector,Permute,Flatten,Multiply\n",
        "from keras.models import Sequential\n",
        "from keras.models import load_model,Model\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "np.random.seed(123)\n",
        "\n",
        "#Read data into pandas dataframe\n",
        "df=pd.read_csv('./sample_data/ratings_Beauty.csv')\n",
        "df=df[['UserId', 'ProductId','Rating','Timestamp']]\n",
        "\n",
        "#Rename the columns\n",
        "df.columns=['userid', 'item_id', 'rating', 'reviewTime']\n",
        "df=df.sort_values(['reviewTime'],ascending=[True])\n",
        "\n",
        "#Divide the train and test data\n",
        "#取数据0-1500行为训练集，1500-2000为测试集\n",
        "\n",
        "length=1500\n",
        "train_end = 2000\n",
        "train_data=df[:length]\n",
        "test_data=df[length:train_end]\n",
        "print(\"train_data.shape, test_data.shape\")\n",
        "print(train_data.shape, test_data.shape)\n",
        "train_data = train_data.reset_index()\n",
        "test_data = test_data.reset_index()\n",
        "    \n",
        "#find item sequences by user in train and test\n",
        "#按用户id排序，统计每个用户购买商品（item_id）的情况，将同一个用户购买的商品（item_id）放入同一个list，所有list组成一个array\n",
        "item_seq_train=train_data.groupby(\"userid\")['item_id'].apply(list).values\n",
        "item_seq_test=test_data.groupby(\"userid\")['item_id'].apply(list).values\n",
        "print(\"item_seq_train.shape\")\n",
        "print(item_seq_train.shape)\n",
        "print(\"item_seq_test.shape\")\n",
        "print(item_seq_test.shape)\n",
        "print(\"item_seq_test[0]\")\n",
        "print(item_seq_test[0])\n",
        "# We consider the minimum sequence length as 6. If length is less than 6, we append dummy ids to the sequence  \n",
        "# 一个用户购买商品数不足6个时，用'padding_id'填充到6个\n",
        "def min_six_len_seq(dictList):\n",
        "    new_list=[]\n",
        "    for i in range(0,len(dictList)):\n",
        "        if len(dictList[i])<6:\n",
        "            w=6-len(dictList[i])\n",
        "            dictList[i]=['padding_id']*w+dictList[i]\n",
        "    return dictList\n",
        "\n",
        "\n",
        "#find the train and test sequences of min len 6\n",
        "item_train = min_six_len_seq(item_seq_train)\n",
        "item_test = min_six_len_seq(item_seq_test)\n",
        "print(\"item_test[5]\")\n",
        "print(item_test[5])\n",
        "\n",
        "# train word2vec model on train_data to get item embeddings\n",
        "#Word2Vec(train_data,size,window,min_count,iter)\n",
        "#size是指词向量的维度，默认为100\n",
        "#min_count:要计算词向量的最小词频。这个值可以去掉一些很生僻的低频词，默认是5.可以对字典做截断， 词频少于min_count次数的单词会被丢弃掉\n",
        "#window:即词向量上下文最大距离,默认为5。window越大，则和某一词较远的词也会产生上下文关系，一般为[5,10]\n",
        "#iter: 随机梯度下降法中迭代的最大次数，默认是5。对于大语料，可以增大这个值\n",
        "#workers：用于控制训练的并行数。\n",
        "def word2vec_model(train_data):\n",
        "    model = Word2Vec(train_data,size=50,window = 3,min_count =1,iter=5)\n",
        "    return model\n",
        "\n",
        "\n",
        "#train the model and save\n",
        "wv_model=word2vec_model(item_train)\n",
        "wv_model.save('word2vec_model')\n",
        "\n",
        "#find full vocabulary\n",
        "entire_products=[]\n",
        "i = 0\n",
        "for key,value in wv_model.wv.vocab.items():\n",
        "    entire_products.append(key)\n",
        "np.save('./entire_products.npy',entire_products)\n",
        "\n",
        "# Divide the sequences into length of 6. ( First 5 items are for train, 6th one for target)\n",
        "#用5个历史购买商品预测下一个商品\n",
        "def input_sequences(new_list, win_size=5):\n",
        "    input_seq=[]\n",
        "    target=[]\n",
        "    for i in range(0,len(new_list)):\n",
        "        seq_len = len(new_list[i])\n",
        "        for j in range(0,seq_len):\n",
        "            if j+win_size<seq_len:\n",
        "                if new_list[i][j+5] in entire_products:\n",
        "                    input_seq.append(new_list[i][j:j+5])\n",
        "                    target.append(new_list[i][j+5])\n",
        "    return input_seq,target\n",
        "\n",
        "\n",
        "# Encoding the target.If new item arrives which i\n",
        "# 将目标商品列表target(原始数据是商品id)转换成商品词汇库中的序号\n",
        "def num_products(target):\n",
        "    product_label=LabelEncoder()\n",
        "    product_label.fit(entire_products)\n",
        "    target_int = product_label.transform(target)\n",
        "    return target_int\n",
        "    \n",
        "#Create Train and test input and target sequences\n",
        "#train_x:5个历史购买商品id list组成的arrary,train_y:目标商品在商品词汇库中对应的序号list\n",
        "train_x,target_train=input_sequences(item_seq_train)\n",
        "train_y=num_products(target_train)\n",
        "\n",
        "test_x,target_test=input_sequences(item_seq_test)\n",
        "test_y=num_products(target_test)\n",
        "    \n",
        "#represent each item with prod2vec embedding and if new item comes in test set, represent it with random vec\n",
        "#将train_x中的每一个商品id都映射成一个维度为50的向量\n",
        "unknown_item_id=np.random.random((50,))\n",
        "def w2v_data_extraction(new_list):\n",
        "    w2v_data=[]\n",
        "    for i in range(0,len(new_list)):\n",
        "        seq_vec=[]\n",
        "        for j in range(0,len(new_list[i])):\n",
        "            try:\n",
        "                embedding=wv_model.wv[new_list[i][j]]\n",
        "            except KeyError:\n",
        "                embedding=unknown_item_id\n",
        "            seq_vec.append(embedding)\n",
        "                \n",
        "        w2v_data.append(seq_vec)\n",
        "    return np.asarray(w2v_data)\n",
        "\n",
        "\n",
        "train_x_emb=w2v_data_extraction(train_x)\n",
        "test_x_emb=w2v_data_extraction(test_x)\n",
        "print(\"train_x_emb.shape\")\n",
        "print(train_x_emb.shape)\n",
        "#model architecture\n",
        "def model_arch():\n",
        "    main_input = Input(shape=(5,50), name='main_input')\n",
        "    lstm_out = LSTM(32)(main_input)\n",
        "    attention = Dense(1, activation='tanh')(lstm_out)\n",
        "    attention = Activation('softmax')(attention)\n",
        "    attention = RepeatVector(32)(attention)\n",
        "    attention = Permute([2, 1])(attention)\n",
        "    attention = Flatten()(attention)\n",
        "    attention_mul = Multiply()([lstm_out, attention])\n",
        "    main_output = (Dense(total_vocab, activation='softmax', name='main_output')(attention_mul))\n",
        "    model = Model(inputs=main_input, outputs=main_output)\n",
        "    model.compile(loss='categorical_crossentropy',metrics=['accuracy'],optimizer='ADAM')\n",
        "    return model\n",
        "\n",
        "\n",
        "#represent output as one-hot encoded\n",
        "def one_hot(seq,total_vocab):\n",
        "    seq_one_hot=np.zeros([len(seq),total_vocab])\n",
        "    for i in range(0,len(seq)):\n",
        "        seq_one_hot[i][seq[i]]=1\n",
        "    return seq_one_hot\n",
        "\n",
        "#fit the model on our data\n",
        "def model_fit(model,train_x,train_y,total_vocab):\n",
        "    print(\"train_x.shape\")\n",
        "    print(train_x.shape)\n",
        "    print(\"train_y\")\n",
        "    print(train_y.shape)\n",
        "    train_y=one_hot(train_y,total_vocab)\n",
        "    print(\"after one_hot train_y,total_vocab\")\n",
        "    print(train_y.shape)\n",
        "    print(total_vocab)\n",
        "\n",
        "    print(\"model is building\")\n",
        "    model.fit(batch_size=64,epochs=10,x=train_x,y=train_y)\n",
        "    print(\"model building done\")\n",
        "    model.save('keras_model.h5')\n",
        "    \n",
        "    return model\n",
        "\n",
        "\n",
        "total_vocab=12102\n",
        "model = model_arch()\n",
        "model=model_fit(model,train_x_emb,train_y,total_vocab)\n",
        "\n",
        "# Hit rate at 1 on test data\n",
        "def hit_rate_at_1(prediction,actual):\n",
        "    return accuracy_score(prediction,actual)\n",
        "\n",
        "\n",
        "# Hit rata at 5 on test data\n",
        "def hit_rate_at_5(pred,actual):\n",
        "    predics = []\n",
        "    for i in range(0, len(pred)):\n",
        "        predics.append(np.argsort(pred[i])[-5:])\n",
        "    count = 0\n",
        "    for i in range(0, len(predics)):\n",
        "        if actual[i] in predics[i]:\n",
        "            count = count + 1\n",
        "\n",
        "    return count/len(actual)\n",
        "\n",
        "\n",
        "# Hit rate at 10 on test data\n",
        "def hit_rate_at_10(pred, actual):\n",
        "    predics = []\n",
        "    for i in range(0, len(pred)):\n",
        "        predics.append(np.argsort(pred[i])[-10:])\n",
        "    count = 0\n",
        "    for i in range(0, len(predics)):\n",
        "        if actual[i] in predics[i]:\n",
        "            count = count + 1\n",
        "    return count /len(actual)\n",
        "    \n",
        "# Prediction on test data\n",
        "def model_predict(model,test_x,test_seq):\n",
        "    pred=model.predict(x=test_x)\n",
        "    preddy=np.argmax(a=pred,axis=1)\n",
        "\n",
        "    print(hit_rate_at_1(preddy,test_seq))\n",
        "    print(hit_rate_at_5(pred, test_seq))\n",
        "    print(hit_rate_at_10(pred, test_seq))\n",
        "\n",
        "#predict on test data\n",
        "model_predict(model,test_x,test_y)\n",
        "print(\"Done\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame( {'a':['A','A','B','B','F','C'], 'b':[1,2,5,5,4,6], 'c':[1,4,6,0,6,2]})\n",
        "print(df)\n",
        "ob = df.groupby('a')['b'].apply(list).values\n",
        "print(ob.shape)"
      ],
      "metadata": {
        "id": "bZ3lDrNFaabR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "X7wqn87MbojL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "le = LabelEncoder()\n",
        "le.fit([\"paris\", \"paris\", \"tokyo\", \"amsterdam\"])\n",
        "print(list(le.classes_))\n",
        "print(le.transform([\"tokyo\", \"tokyo\", \"paris\"]))\n",
        "print(list(le.inverse_transform([2, 2, 1])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tfAHdmY2zEPt",
        "outputId": "f886f76e-df65-460a-f9a8-5793c7ac1442"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['amsterdam', 'paris', 'tokyo']\n",
            "[2 2 1]\n",
            "['tokyo', 'tokyo', 'paris']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip list"
      ],
      "metadata": {
        "id": "fZ_NhomBMIFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "a = np.array([3, 1, 2, 4, 6, 1])\n",
        "print(np.argmax(a))"
      ],
      "metadata": {
        "id": "HYAu50EiQ9c6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "a = np.array([[1, 5, 5, 2],\n",
        "        [9, 6, 2, 8],\n",
        "        [3, 7, 9, 1]])\n",
        "print(np.argmax(a, axis=0))"
      ],
      "metadata": {
        "id": "om2XVX0IRDRB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "a = np.array([[1, 5, 5, 2],\n",
        "        [9, 6, 2, 8],\n",
        "        [3, 7, 9, 1]])\n",
        "print(np.argmax(a, axis=1))"
      ],
      "metadata": {
        "id": "jmsfT6AqRNSx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}