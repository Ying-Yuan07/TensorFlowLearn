{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "app使用推荐.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMYu/Jlv59YYIDeADZe3QyE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ying-Yuan07/TensorFlowLearn/blob/main/app%E4%BD%BF%E7%94%A8%E6%8E%A8%E8%8D%90.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jGUzwtqsGUUM",
        "outputId": "eef06060-d744-44ec-8c57-bd0f3524ef1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "values.shape\n",
            "(295310, 4)\n",
            "[[  0  19   0   1]\n",
            " [  0   0   0   1]\n",
            " [  0   8   0   1]\n",
            " ...\n",
            " [291   4   0   3]\n",
            " [291   4   0   3]\n",
            " [291  17   0   3]]\n",
            "        var1(t-9)  var2(t-9)  var3(t-9)  var4(t-9)  var1(t-8)  var2(t-8)  var3(t-8)  var4(t-8)  var1(t-7)  var2(t-7)  var3(t-7)  var4(t-7)  var1(t-6)  var2(t-6)  var3(t-6)  var4(t-6)  var1(t-5)  var2(t-5)  var3(t-5)  var4(t-5)  var1(t-4)  var2(t-4)  var3(t-4)  var4(t-4)  var1(t-3)  var2(t-3)  var3(t-3)  var4(t-3)  var1(t-2)  var2(t-2)  var3(t-2)  var4(t-2)  var1(t-1)  var2(t-1)  var3(t-1)  var4(t-1)  var1(t)  var2(t)  var3(t)  var4(t)\n",
            "0             NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN      0.0     0.95      0.0      0.2\n",
            "1             NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        0.0       0.95        0.0        0.2      0.0     0.00      0.0      0.2\n",
            "2             NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        0.0       0.95        0.0        0.2        0.0       0.00        0.0        0.2      0.0     0.40      0.0      0.2\n",
            "3             NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        0.0       0.95        0.0        0.2        0.0       0.00        0.0        0.2        0.0       0.40        0.0        0.2      0.0     0.05      0.0      0.2\n",
            "4             NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        0.0       0.95        0.0        0.2        0.0       0.00        0.0        0.2        0.0       0.40        0.0        0.2        0.0       0.05        0.0        0.2      0.0     0.05      0.0      0.2\n",
            "...           ...        ...        ...        ...        ...        ...        ...        ...        ...        ...        ...        ...        ...        ...        ...        ...        ...        ...        ...        ...        ...        ...        ...        ...        ...        ...        ...        ...        ...        ...        ...        ...        ...        ...        ...        ...      ...      ...      ...      ...\n",
            "295305        1.0       0.85        0.0        0.6        1.0       0.85        0.0        0.6        1.0       0.85        0.0        0.6        1.0       0.95        0.0        0.6        1.0       0.85        0.0        0.6        1.0       0.85        0.0        0.6        1.0       0.85        0.0        0.6        1.0       0.85        0.0        0.6        1.0       0.85        0.0        0.6      1.0     0.85      0.0      0.6\n",
            "295306        1.0       0.85        0.0        0.6        1.0       0.85        0.0        0.6        1.0       0.95        0.0        0.6        1.0       0.85        0.0        0.6        1.0       0.85        0.0        0.6        1.0       0.85        0.0        0.6        1.0       0.85        0.0        0.6        1.0       0.85        0.0        0.6        1.0       0.85        0.0        0.6      1.0     0.85      0.0      0.6\n",
            "295307        1.0       0.85        0.0        0.6        1.0       0.95        0.0        0.6        1.0       0.85        0.0        0.6        1.0       0.85        0.0        0.6        1.0       0.85        0.0        0.6        1.0       0.85        0.0        0.6        1.0       0.85        0.0        0.6        1.0       0.85        0.0        0.6        1.0       0.85        0.0        0.6      1.0     0.20      0.0      0.6\n",
            "295308        1.0       0.95        0.0        0.6        1.0       0.85        0.0        0.6        1.0       0.85        0.0        0.6        1.0       0.85        0.0        0.6        1.0       0.85        0.0        0.6        1.0       0.85        0.0        0.6        1.0       0.85        0.0        0.6        1.0       0.85        0.0        0.6        1.0       0.20        0.0        0.6      1.0     0.20      0.0      0.6\n",
            "295309        1.0       0.85        0.0        0.6        1.0       0.85        0.0        0.6        1.0       0.85        0.0        0.6        1.0       0.85        0.0        0.6        1.0       0.85        0.0        0.6        1.0       0.85        0.0        0.6        1.0       0.85        0.0        0.6        1.0       0.20        0.0        0.6        1.0       0.20        0.0        0.6      1.0     0.85      0.0      0.6\n",
            "\n",
            "[295310 rows x 40 columns]\n",
            "(295301, 40)\n",
            "295301\n",
            "206710\n",
            "(206710, 36) 206710 (206710,)\n",
            "(206710, 9, 4) (206710,) (88591, 9, 4) (88591,)\n",
            "Epoch 1/10\n",
            "2871/2871 - 18s - loss: 0.3653 - val_loss: 0.0163 - 18s/epoch - 6ms/step\n",
            "Epoch 2/10\n",
            "2871/2871 - 15s - loss: 0.3653 - val_loss: 0.0163 - 15s/epoch - 5ms/step\n",
            "Epoch 3/10\n",
            "2871/2871 - 15s - loss: 0.3653 - val_loss: 0.0163 - 15s/epoch - 5ms/step\n",
            "Epoch 4/10\n",
            "2871/2871 - 14s - loss: 0.3653 - val_loss: 0.0163 - 14s/epoch - 5ms/step\n",
            "Epoch 5/10\n",
            "2871/2871 - 15s - loss: 0.3653 - val_loss: 0.0163 - 15s/epoch - 5ms/step\n",
            "Epoch 6/10\n",
            "2871/2871 - 15s - loss: 0.3653 - val_loss: 0.0163 - 15s/epoch - 5ms/step\n",
            "Epoch 7/10\n",
            "2871/2871 - 15s - loss: 0.3653 - val_loss: 0.0163 - 15s/epoch - 5ms/step\n",
            "Epoch 8/10\n",
            "2871/2871 - 15s - loss: 0.3653 - val_loss: 0.0163 - 15s/epoch - 5ms/step\n",
            "Epoch 9/10\n",
            "2871/2871 - 15s - loss: 0.3653 - val_loss: 0.0163 - 15s/epoch - 5ms/step\n",
            "Epoch 10/10\n",
            "2871/2871 - 15s - loss: 0.3653 - val_loss: 0.0163 - 15s/epoch - 5ms/step\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-468353521732>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    131\u001b[0m model.fit(train_X, train_y, epochs=10, batch_size=72,\n\u001b[1;32m    132\u001b[0m                       \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m                       callbacks=[metrics])\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-468353521732>\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         val_predict = (numpy.asarray(self.model.predict(\n\u001b[0;32m---> 44\u001b[0;31m             self.validation_data[0]))).round()\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0mval_targ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0m_val_f1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_targ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_predict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
          ]
        }
      ],
      "source": [
        "from math import sqrt\n",
        "from numpy import concatenate\n",
        "from matplotlib import pyplot\n",
        "from pandas import read_csv\n",
        "from pandas import DataFrame\n",
        "from pandas import concat\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Activation\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "import numpy\n",
        "from keras.callbacks import Callback\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "\n",
        "\"\"\"\n",
        "本文是LSTM多元预测：app\n",
        "用3个步长的数据预测1个步长的数据\n",
        "包含：\n",
        "对数据进行缩放，缩放格式为n行*4列\n",
        "构造3->1的监督学习数据\n",
        "构造网络开始预测\n",
        "将预测结果重新拼接为n行*8列数据\n",
        "数据逆缩放，求RSME误差\n",
        "\"\"\"\n",
        "pd.set_option('display.max_columns',1000)\n",
        "pd.set_option('display.width', 1000)\n",
        "pd.set_option('display.max_colwidth',1000)\n",
        "\n",
        "class Metrics(Callback):\n",
        "    def on_train_begin(self, logs={}):\n",
        "        self.val_f1s = []\n",
        "        self.val_recalls = []\n",
        "        self.val_precisions = []\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        val_predict = (numpy.asarray(self.model.predict(\n",
        "            self.validation_data[0]))).round()\n",
        "        val_targ = self.validation_data[1]\n",
        "        _val_f1 = f1_score(val_targ, val_predict)\n",
        "        _val_recall = recall_score(val_targ, val_predict)\n",
        "        _val_precision = precision_score(val_targ, val_predict)\n",
        "        self.val_f1s.append(_val_f1)\n",
        "        self.val_recalls.append(_val_recall)\n",
        "        self.val_precisions.append(_val_precision)\n",
        "        return\n",
        "\n",
        "# 转换成监督数据，四列数据，3->1，三组预测一组\n",
        "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
        "    n_vars = 1 if type(data) is list else data.shape[1]\n",
        "    df = DataFrame(data)\n",
        "    cols, names = list(), list()\n",
        "    # input sequence (t-n, ... t-1)\n",
        "    # 将3组输入数据依次向下移动3，2，1行，将数据加入cols列表（技巧：(n_in, 0, -1)中的-1指倒序循环，步长为1）\n",
        "    for i in range(n_in, 0, -1):\n",
        "    \tcols.append(df.shift(i))\n",
        "    \tnames += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
        "    # forecast sequence (t, t+1, ... t+n)\n",
        "    # 将一组输出数据加入cols列表（技巧：其中i=0）\n",
        "    for i in range(0, n_out):\n",
        "    \tcols.append(df.shift(-i))\n",
        "    \tif i == 0:\n",
        "    \t\tnames += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
        "    \telse:\n",
        "    \t\tnames += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
        "    # cols列表(list)中现在有四块经过下移后的数据(即：df(-3),df(-2),df(-1),df)，将四块数据按列 并排合并\n",
        "    agg = concat(cols, axis=1)\n",
        "    # 给合并后的数据添加列名\n",
        "    agg.columns = names\n",
        "    print(agg)\n",
        "    # 删除NaN值列\n",
        "    if dropnan:\n",
        "    \tagg.dropna(inplace=True)\n",
        "    return agg\n",
        "\n",
        "# load dataset\n",
        "dataset = read_csv('./sample_data/username.csv', header=0)\n",
        "values = dataset.values\n",
        "print(\"values.shape\")\n",
        "print(values.shape)\n",
        "print(values)\n",
        "values = values.astype('float32')\n",
        "# 标准化/放缩 特征值在（0,1）之间\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "scaled = scaler.fit_transform(values)\n",
        "# 用3小时数据预测一小时数据，4个特征值\n",
        "n_hours = 9\n",
        "n_features = 4\n",
        "# 构造一个3->1的监督学习型数据\n",
        "reframed = series_to_supervised(scaled, n_hours, 1)\n",
        "print(reframed.shape)\n",
        "\n",
        "# split into train and test sets\n",
        "values = reframed.values\n",
        "# 用一年的数据来训练\n",
        "n_train_hours = int(values.shape[0]* 0.7)\n",
        "print(values.shape[0])\n",
        "print(n_train_hours)\n",
        "\n",
        "\n",
        "\n",
        "train = values[:n_train_hours, :]\n",
        "test = values[n_train_hours:, :]\n",
        "# split into input and outputs\n",
        "n_obs = n_hours * n_features\n",
        "# 有16=(4*4)列数据，取前12=(3*4) 列作为X，倒数第4列=(第13列)作为Y\n",
        "train_X, train_y = train[:, :n_obs], train[:, -n_features]\n",
        "test_X, test_y = test[:, :n_obs], test[:, -n_features]\n",
        "print(train_X.shape, len(train_X), train_y.shape)\n",
        "# 将数据转换为3D输入，timesteps=3，3条数据预测1条 [samples, timesteps, features]\n",
        "train_X = train_X.reshape((train_X.shape[0], n_hours, n_features))\n",
        "test_X = test_X.reshape((test_X.shape[0], n_hours, n_features))\n",
        "print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)\n",
        "\n",
        "# 设计网络\n",
        "model = Sequential()\n",
        "model.add(LSTM(10, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
        "model.add(Dense(1))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.compile(loss='mse', optimizer='adam')\n",
        "# 拟合网络\n",
        "history = model.fit(train_X, train_y, epochs=10, batch_size=72, validation_data=(test_X, test_y), verbose=2, shuffle=False)\n",
        "metrics = Metrics()\n",
        "model.fit(train_X, train_y, epochs=10, batch_size=72,\n",
        "                      verbose=0, validation_data=(test_X, test_y),\n",
        "                      callbacks=[metrics])\n",
        "print(metrics)\n",
        "\n",
        "# plot history\n",
        "pyplot.plot(history.history['loss'], label='train')\n",
        "pyplot.plot(history.history['val_loss'], label='test')\n",
        "pyplot.legend()\n",
        "pyplot.show()\n",
        "\n",
        "# 执行预测\n",
        "yhat = model.predict(test_X)\n",
        "# 将数据格式化成 n行 * 24列\n",
        "test_X = test_X.reshape((test_X.shape[0], n_hours*n_features))\n",
        "# 将预测列据和后3列数据拼接，因后续逆缩放时，数据形状要符合 n行*8列 的要求\n",
        "inv_yhat = concatenate((yhat, test_X[:, -3:]), axis=1)\n",
        "# 对拼接好的数据进行逆缩放\n",
        "inv_yhat = scaler.inverse_transform(inv_yhat)\n",
        "inv_yhat = inv_yhat[:,0]\n",
        "\n",
        "test_y = test_y.reshape((len(test_y), 1))\n",
        "# 将真实列据和后3列数据拼接，因后续逆缩放时，数据形状要符合 n行*8列 的要求\n",
        "inv_y = concatenate((test_y, test_X[:, -3:]), axis=1)\n",
        "# 对拼接好的数据进行逆缩放\n",
        "inv_y = scaler.inverse_transform(inv_y)\n",
        "inv_y = inv_y[:,0]\n",
        "\n",
        "# 计算RMSE误差值\n",
        "rmse = sqrt(mean_squared_error(inv_y, inv_yhat))\n",
        "print('Test RMSE: %.3f' % rmse)"
      ]
    }
  ]
}